# Python de alto rendimiento

Python se pensó para ocultar los detalles de implementación del lenguaje y que
el desarrollador pudiera centrarse en los aspectos prácticos de la resolución
de problemas.

Desafortunadamente, la abstracción está reñida con el rendimiento y los
mecanismos que empleamos para mantener un diseño modular, reutilizable
y cohesionado, pero sin acoplamiento o dependencias fuertes, añaden una
sobrecarga que puede ser evidente en ciertas situaciones. Al fin y al cabo los
procesadores se diseñaron para optimizar la ejecución de código ensamblador,
no Python.

Los optimizadores de código de los lenguajes compilados hacen precisamente
esto: eliminan la abstracción y acoplan los subsistemas del lenguaje tomando
decisiones que suelen intercambiar especio por velocidad.

Afortunadamente para nosotros, la compilación no es el único recurso. La
naturaleza misma del problema puede arrojar pistas de mejores estrategias
para resolver el mismo, sin tener que descender al mundo de los lenguajes
compilados.

## Caracterización de aplicaciones (_profiling_)

Para poder optimizar un problema, primero debemos caracterizarlo. Existen
algunas herramientas en el ecosistema Python para realizar este análisis y
el propio Python incluye el módulo `cProfiler` para este fin.

1. Crea un fichero `profiling.py` y añade el siguiente listado:

    ```python
    import time

    def improvable():
        for i in range(10000000):
            _ = i**2
        time.sleep(2)

    def not_improvable():
        time.sleep(1)

    def main():
        not_improvable()
        improvable()

    if __name__ == '__main__':
        main()
    ```

2. Ahora, desde una línea de comandos, ejecuta:

    ```bash
    $ python -m cProfile profiling.py
    ```

    El resultado tendrá una pinta similar a:

    ```bash
    $ python -m cProfile profiling.py
             8 function calls in 6.117 seconds

       Ordered by: standard name

       ncalls  tottime  percall  cumtime  percall filename:lineno(function)
            1    0.000    0.000    6.117    6.117 profiling.py:1(<module>)
            1    0.000    0.000    6.117    6.117 profiling.py:11(main)
            1    3.108    3.108    5.112    5.112 profiling.py:3(improvable)
            1    0.000    0.000    1.004    1.004 profiling.py:8(not_improvable)
            1    0.000    0.000    6.117    6.117 {built-in method builtins.exec}
            2    3.008    1.504    3.008    1.504 {built-in method time.sleep}
            1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
    ```

    Donde existe una fila por cada función involucrada en la ejecución y
    los campos significan:

    - `ncalls` es el número de llamadas a la función.
    - `tottime` es el tiempo total pasado en esa función, sin contar el tiempo
    pasado en las sub-llamadas.
    - `percall` es la media de tiempo total por llamada.
    - `cumtime` es el tiempo total empleado en esa función, incluyendo
    sub-llamadas.
    - `percall` es la media del tiempo acumulado por llamada.
    - `filename:lineno(function)` indica dónde se produce la inversión de
    tiempo.

3. El módulo `cPython` permite grabar exportar esta información:

    ```bash
    $ python -m cProfile -o data.prof profiling.py
    ```

    Y utilizar
    [SnakeViz](https://jiffyclub.github.io/snakeviz/) o
    [tuna](https://github.com/nschloe/tuna)
    para visualizar el resultado:

    ```bash
    $ pip install tuna snakeviz
    $ snakeviz data.prof
    ```

    Cuidado, porque en las interfaces gráficas puede darse una interpretación
    distinta a los mismos nombre. La mayoría llama "tiempo total" al tiempo
    invertido por una función y las sub-llamadas. Cuando esto pasa, el tiempo
    empleado sólo en la función se suele llamar "tiempo local" o se marca de
    alguna forma.

Los gráficos de llamas son útiles porque permiten visualizar fracciones de
tiempo. En lenguajes interpretados, la mayor parte del fondo de las gráficas
son llamadas a las funciones de la biblioteca estándar, muchas de ellas
nativas y, por tanto, "inmejorables".

Conviene primero buscar las funciones que consumen más tiempo acumulado y,
de entre ellas, comenzar por las que emplean más tiempo en sí mismas.

## La ley de Amdahl

La ley de Amdahl dice:

> La mejora en la latencia de un sistema está limitada por la porción de
> tiempo en que no obtenemos ninguna ganancia.

La ley de Amdahl se expresa matemáticamente como `1/(1 - p)` donde `p` es la
**fracción de tiempo en la que podemos aplicar la mejora**.

Por ejemplo, supón que caracterizamos un software y descubrimos que el 80%
del tiempo lo empleamos en una función fácilmente paralelizable. Cuestión de
arrojar más núcleos al problema. Lo que dice la ley de Amdahl es que la mejora
en velocidad está limitada por ese 20% en el que no podemos aplicar la mejora.
En particular, la mejora no puede ser superior a `1/(1 - 0.8) = 5`. O lo que
es lo mismo, **el sistema nunca será más de 5 veces más rápido**.

La mejora real del sistema se calcula en base a la mejora local, según la
expresión `S = 1/((1 - p) + p/s)` donde `S` es la mejora global (_global
speedup_) y `p` vuelve a ser la fracción del tiempo donde podemos aplicar
la mejora local `s` (_local speedup_).

## Paralelismo y multiprocesamiento

¿Permite Python ejecución paralela?

La respuesta rápida es no. Sin embargo podemos ser algo más transigentes y
plantearnos otra pregunta, más general:

¿Permite Python ejecución concurrente?

En este caso **la respuesta es sí**. La diferencia entre paralelismo y
concurrencia es sutil aunque relevante. La **concurrencia se produce cuando
más de una tarea progresa al mismo tiempo**, sin esperar las unas a la
finalización de las otras. El **paralelismo se produce cuando dos tareas se
ejecutan al mismo tiempo**.

Python **soporta múltiples tareas (threads) pero sólo una de ellas puede estar
ejecutándose a la vez en el intérprete**. Quien posee la ejecución en cada
momento, se dice que está en posesión del
[GIL](https://wiki.python.org/moin/GlobalInterpreterLock).
Sin embargo, Python interrumpirá las tareas automáticamente y bajo ciertas
condiciones para dejar que otras tareas progresen antes de que la actual
termine.

Ojo, es el intérprete de Python el que no puede ejecutar más de una tarea al
mismo tiempo. Si la ejecución ha salido fuera del intérprete (a una llamada
del sistema nativo en C, por ejemplo), esta restricción no aplica y la
ejecución depende del sistema.

* [GIL: Todo lo que quisiste saber y no te atreviste a preguntar](https://www.youtube.com/watch?v=50eOwz9lek4&list=PLKxa4AIfm4pUQX9ePOy3KEpENDC331Izi&index=36)
* La [asincronía ilustrada en un caso de WebVR](https://hacks.mozilla.org/2017/11/a-super-stable-webvr-user-experience-thanks-to-firefox-quantum/).

### Concurrencia multi-hilo

1. Reemplaza el código de `profiling.py` (o crea un fichero nuevo) con
este otro:

    ```python
    from urllib import request

    def get(url):
        response = request.urlopen(url)
        print(f'Body at {url}:\n {response.read()}')

    def main():
        urls = [
            'https://raw.githubusercontent.com/python/cpython/master/README.rst',
            'https://raw.githubusercontent.com/rust-lang/rust/master/README.md',
            'https://raw.githubusercontent.com/ruby/ruby/master/README.md'] * 80

        list(map(get, urls))

    if __name__ == '__main__':
        main()
    ```

    Caracterízalo y visualiza los resultados. ¿Qué observas?

2. Ahora cambia el código para que utilice hilos:

    ```python
    from urllib import request
    from concurrent.futures import ThreadPoolExecutor

    def get(url):
        response = request.urlopen(url)
        print(f'Body at {url}:\n {response.read()}')

    def main():
        urls = [
            'https://raw.githubusercontent.com/python/cpython/master/README.rst',
            'https://raw.githubusercontent.com/rust-lang/rust/master/README.md',
            'https://raw.githubusercontent.com/ruby/ruby/master/README.md'] * 80

        with ThreadPoolExecutor(max_workers=5) as executor:
            executor.map(get, urls)

    if __name__ == '__main__':
        main()
    ```

    Caracterízalo **en un fichero de salida distinto** y visualiza los
    resultados. Compáralos con los de antes.

### Paralelismo multiproceso

1. Considera el siguiente código:

    ```python
    from datetime import datetime
    import math

    PRIMES = [
        112272535095293,
        112582705942171,
        112272535095293,
        115280095190773,
        115797848077099,
        1099726899285419]

    def is_prime(n):
        if n % 2 == 0:
            return False

        sqrt_n = int(math.floor(math.sqrt(n)))
        for i in range(3, sqrt_n + 1, 2):
            if n % i == 0:
                return False
        return True

    def main():
        start = datetime.now()
        for number, prime in zip(PRIMES, map(is_prime, PRIMES)):
            print('%d is prime: %s' % (number, prime))

        print('Elapsed time:', datetime.now() - start)

    if __name__ == '__main__':
        main()
    ```

2. Ahora considera su versión multi-hilo:

    ```python
    from datetime import datetime
    from concurrent.futures import ThreadPoolExecutor
    import math

    PRIMES = [
        112272535095293,
        112582705942171,
        112272535095293,
        115280095190773,
        115797848077099,
        1099726899285419]

    def is_prime(n):
        if n % 2 == 0:
            return False

        sqrt_n = int(math.floor(math.sqrt(n)))
        for i in range(3, sqrt_n + 1, 2):
            if n % i == 0:
                return False
        return True

    def main():
        start = datetime.now()
        with ThreadPoolExecutor(max_workers=2) as executor:
            for number, prime in zip(PRIMES, executor.map(is_prime, PRIMES)):
                print('%d is prime: %s' % (number, prime))

        print('Elapsed time:', datetime.now() - start)

    if __name__ == '__main__':
        main()
    ```

    Caracterízala y observa las diferencias. ¿Qué ha pasado?

3. Ahora prueba con la siguiente versión multiproceso:

    ```python
    from datetime import datetime
    from concurrent.futures import ProcessPoolExecutor
    import math

    PRIMES = [
        112272535095293,
        112582705942171,
        112272535095293,
        115280095190773,
        115797848077099,
        1099726899285419]

    def is_prime(n):
        if n % 2 == 0:
            return False

        sqrt_n = int(math.floor(math.sqrt(n)))
        for i in range(3, sqrt_n + 1, 2):
            if n % i == 0:
                return False
        return True

    def main():
        start = datetime.now()
        with ProcessPoolExecutor(max_workers=2) as executor:
            for number, prime in zip(PRIMES, executor.map(is_prime, PRIMES)):
                print('%d is prime: %s' % (number, prime))

        print('Elapsed time:', datetime.now() - start)

    if __name__ == '__main__':
        main()
    ```

    No intentes caracterizarlo, no funcionará. Es una de las limitaciones
    de cProfile: no funciona con mutiprocesamiento.

4. Ve aumentando el parámetro `processes` desde 2 hasta que no notes ninguna
mejora. ¿Por qué deja de haber mejora? Compara este número con el número
de núcleos de tu ordenador.

### ¿Cuándo usar paralelismo y cuándo usar concurrencia?

En general, tendrás que distinguir si tus programas están constreñidos por la
CPU o por la entrada y salida. Estas situaciones se conocen respectivamente
como [_CPU Bound_](https://en.wikipedia.org/wiki/CPU-bound) e
[_I/O Bound_](https://en.wikipedia.org/wiki/I/O_bound).

Si tu aplicación es _I/O Bound_ pasará más tiempo fuera del intérprete de
Python que dentro. En este caso, cualquiera de las aproximaciones es válida
aunque la concurrencia es más ligera y no implica al sistema operativo.

Si tu aplicación es _CPU Bound_, entonces pasará más tiempo en el intérprete
de Python que fuera. Es lo que pasa con los algoritmos matemáticos de solución
de problemas algebráicos o de cálculo. En este caso tienes que llevarte la
gestión de la multitarea al sistema operativo con multiprocesamiento porque
el intérprete sólo permitirá a un hilo operar al mismo tiempo.

## Compiladores "al vuelo" (_JIT compilers_): Numba

Antes de problar con paralelización, sobre todo si nuestro código es de tipo
"operaciones", convendría probar [Numba](https://numba.pydata.org), una
biblioteca que  implementa un compilador "al vuelo" compatible con la
implementación de referencia, CPython.

Sencillamente importa el decorador `jit` y aplícalo a tu función crítica
(Numba viene incluído en Anaconda, así que puedes usarlo desde el entorno
virtual `base` que instalaste en el tema anterior):

```python
from numba import jit
from datetime import datetime
import math

PRIMES = [
    112272535095293,
    112582705942171,
    112272535095293,
    115280095190773,
    115797848077099,
    1099726899285419]

@jit(nopython=True)
def is_prime(n):
    if n % 2 == 0:
        return False

    sqrt_n = int(math.floor(math.sqrt(n)))
    for i in range(3, sqrt_n + 1, 2):
        if n % i == 0:
            return False
    return True

def main():
    start = datetime.now()
    for number, prime in zip(PRIMES, map(is_prime, PRIMES)):
        print('%d is prime: %s' % (number, prime))

    print('Elapsed time:', datetime.now() - start)

if __name__ == '__main__':
    main()
```

La principal ventaja de NumPy sobre PyPy es que funciona con el intérprete
oficial y que, además, está optimizado para trabajar conjuntamente con NumPy.

Numba es además capaz de paralelizar código automáticamente, compilar para
[SIMD](https://en.wikipedia.org/wiki/SIMD) e, incluso, ¡para GPUs!

* [A ~5 minute guide to Numba](http://numba.pydata.org/numba-doc/latest/user/5minguide.html)*
* [Documentación de Numba](http://numba.pydata.org/)

## Intérpretes más rápidos: PyPy

Otra alternativa es utilizar una implementación de Python más rápida, como
[PyPy](https://pypy.org/). PyPy es, actualmente, compatible con Python 3.6 y
una de sus desventajas es, precisamente, que **suele ir una versión por
detrás del intérprete oficial** y que **no todo el software compatible con
CPython, lo es también con PyPy**.

No obstante, si puedes permitirte ir una versión por detrás y resulta que no
utilizas nada incompatible con el intérprete, la ganancia vale la pena, aunque
[es similar a la de Numba](https://www.ibm.com/developerworks/community/blogs/jfp/entry/A_Comparison_Of_C_Julia_Python_Numba_Cython_Scipy_and_BLAS_on_LU_Factorization?lang=en).

1. Abre una consola y usa pyenv para instalar la última versión de PyPy:

    ```bash
    $ pyenv install pypy3.6-7.1.0
    ```

2. Restaura la versión mono-hilo del comprobador de primos:

    ```python
    from datetime import datetime
    import math

    PRIMES = [
        112272535095293,
        112582705942171,
        112272535095293,
        115280095190773,
        115797848077099,
        1099726899285419]

    def is_prime(n):
        if n % 2 == 0:
            return False

        sqrt_n = int(math.floor(math.sqrt(n)))
        for i in range(3, sqrt_n + 1, 2):
            if n % i == 0:
                return False
        return True

    def main():
        start = datetime.now()
        for number, prime in zip(PRIMES, map(is_prime, PRIMES)):
            print('%d is prime: %s' % (number, prime))

        print('Elapsed time:', datetime.now() - start)

    if __name__ == '__main__':
        main()
    ```

3. Lanza el _script_ desde una terminal:

    ```bash
    $ python profiling.py
    ```

4. Ahora instala y activa la última versión de pypy con pyenv:

    ```bash
    $ pyenv install pypy3.6-7.1.0
    $ pyenv shell pypy3.6-7.1.0
    ```

5. Lanza de nuevo el _script_, una vez activado pypy:

    ```bash
    $ python profiling.py
    ```

* Compatibilidad con los
[1000 paquetes más utilizados de Python](http://packages.pypy.org)

## Extensiones nativas